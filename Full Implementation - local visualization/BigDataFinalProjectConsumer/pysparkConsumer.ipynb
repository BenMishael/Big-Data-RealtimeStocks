{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e965fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4acc733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# App CONSTANTS\n",
    "STATIC_DATA_PATH = \"stocks_static_data.csv\"\n",
    "TRANSACTIONS_TOPIC = \"transactions\"\n",
    "KAFKA_SERVER = \"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffe2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store accumulated data\n",
    "accumulated_data = {\n",
    "    'industry_growth': pd.DataFrame(),\n",
    "    'transaction_types': pd.DataFrame(),\n",
    "    'daily_volume': pd.DataFrame(),\n",
    "    'avg_volume': pd.DataFrame(),\n",
    "    'country_growth': pd.DataFrame(),\n",
    "    'tech_growth': pd.DataFrame()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1632422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for summary\n",
    "total_transactions = 0\n",
    "unique_countries = set()\n",
    "unique_tickers = set()\n",
    "unique_industries = set()\n",
    "unique_sectors = set()\n",
    "total_volume = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439db7ce10c6d780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T09:17:39.585715Z",
     "start_time": "2024-08-23T09:16:56.308497Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockMarketAnalysis\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31db06bee800738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T09:17:39.593539Z",
     "start_time": "2024-08-23T09:17:39.588421Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the static data schema:\n",
    "static_tickers_data_scheme = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"company_name\", StringType(), True),\n",
    "    StructField(\"shares_outstanding\", FloatType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"sector\", StringType(), True),\n",
    "    StructField(\"industry\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])\n",
    "\n",
    "# define mock live transaction data scheme:\n",
    "live_transactions_data_scheme = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07acdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug purposes - check if the csv file exists\n",
    "print(f\"CSV file exists: {os.path.exists(STATIC_DATA_PATH)}\")\n",
    "print(f\"CSV file path: {os.path.abspath(STATIC_DATA_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c68d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the static data\n",
    "tickers_df = spark.read.csv(STATIC_DATA_PATH, header=True, schema=static_tickers_data_scheme)\n",
    "\n",
    "# For debug purposes - Print the schema of the Tickers DataFrame\n",
    "print(\"Tickers DataFrame Schema:\")\n",
    "tickers_df.printSchema()\n",
    "print(\"\\nFirst few rows of the Tickers DataFrame:\")\n",
    "tickers_df.show(5, truncate=False)\n",
    "print(f\"\\nNumber of rows in Tickers DataFrame: {tickers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa279d91e744f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T09:17:41.981914Z",
     "start_time": "2024-08-23T09:17:39.595108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Kafka stream\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "    .option(\"subscribe\", TRANSACTIONS_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 1000) \\\n",
    "    .load()\n",
    "\n",
    "# For debug purposes - Print Kafka DataFrame schema\n",
    "print(\"Kafka DataFrame Schema:\")\n",
    "kafka_df.printSchema()\n",
    "\n",
    "# Parse the JSON data\n",
    "parsed_df = kafka_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), live_transactions_data_scheme).alias(\"parsed_data\")\n",
    ").select(\n",
    "    col(\"parsed_data.ticker\"),\n",
    "    to_timestamp(col(\"parsed_data.timestamp\"), \"dd/MM/yyyy HH:mm\").alias(\"timestamp\"),\n",
    "    col(\"parsed_data.price\").cast(\"float\").alias(\"price\"),\n",
    "    col(\"parsed_data.volume\").cast(\"integer\").alias(\"volume\"),\n",
    "    col(\"parsed_data.action\"),\n",
    "    to_date(to_timestamp(col(\"parsed_data.timestamp\"), \"dd/MM/yyyy HH:mm\")).alias(\"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plots\n",
    "plt.ion()\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 20))\n",
    "fig.suptitle('Stock Market Analysis', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f3d1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots(df):\n",
    "    global accumulated_data\n",
    "    \n",
    "    # 1. Industry growth analysis\n",
    "    new_industry_growth = df.groupBy(\"industry\").agg(sum(\"volume\").alias(\"total_volume\")).toPandas()\n",
    "    accumulated_data['industry_growth'] = pd.concat([accumulated_data['industry_growth'], new_industry_growth]).groupby('industry').sum().reset_index()\n",
    "\n",
    "    # 2. Percentage of each transaction type\n",
    "    new_transaction_types = df.groupBy(\"action\").count().toPandas()\n",
    "    accumulated_data['transaction_types'] = pd.concat([accumulated_data['transaction_types'], new_transaction_types]).groupby('action').sum().reset_index()\n",
    "\n",
    "    # 3. Daily volume for each stock\n",
    "    new_daily_volume = df.groupBy(\"ticker\", \"date\").agg(sum(\"volume\").alias(\"daily_volume\")).toPandas()\n",
    "    accumulated_data['daily_volume'] = pd.concat([accumulated_data['daily_volume'], new_daily_volume])\n",
    "    accumulated_data['daily_volume'] = accumulated_data['daily_volume'].groupby(['ticker', 'date']).sum().reset_index()\n",
    "\n",
    "    # 4. Stock with highest average trading volume\n",
    "    new_avg_volume = df.groupBy(\"ticker\").agg(avg(\"volume\").alias(\"avg_volume\")).toPandas()\n",
    "    accumulated_data['avg_volume'] = pd.concat([accumulated_data['avg_volume'], new_avg_volume]).groupby('ticker').mean().reset_index()\n",
    "\n",
    "    # 5. Locations of growing companies by country\n",
    "    new_country_growth = df.groupBy(\"country\").agg(sum(\"volume\").alias(\"total_volume\")).toPandas()\n",
    "    accumulated_data['country_growth'] = pd.concat([accumulated_data['country_growth'], new_country_growth]).groupby('country').sum().reset_index()\n",
    "\n",
    "    # 6. Growing technologies market\n",
    "    new_tech_growth = df.filter(col(\"sector\") == \"Technology\").groupBy(\"industry\").agg(sum(\"volume\").alias(\"total_volume\")).toPandas()\n",
    "    accumulated_data['tech_growth'] = pd.concat([accumulated_data['tech_growth'], new_tech_growth]).groupby('industry').sum().reset_index()\n",
    "\n",
    "    # Clear the current output and create new plots\n",
    "    clear_output(wait=True)\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(20, 20))\n",
    "    fig.suptitle('Stock Market Analysis', fontsize=16)\n",
    "\n",
    "    # Plot accumulated data\n",
    "    sns.heatmap(accumulated_data['industry_growth'].pivot(columns='industry', values='total_volume'), ax=axs[0, 0], cmap='YlOrRd')\n",
    "    axs[0, 0].set_title('Industry Growth')\n",
    "\n",
    "    sns.barplot(x='action', y='count', data=accumulated_data['transaction_types'], ax=axs[0, 1])\n",
    "    axs[0, 1].set_title('Transaction Types')\n",
    "\n",
    "    sns.lineplot(x='date', y='daily_volume', hue='ticker', data=accumulated_data['daily_volume'], ax=axs[1, 0])\n",
    "    axs[1, 0].set_title('Daily Volume by Stock')\n",
    "\n",
    "    top_10_volume = accumulated_data['avg_volume'].nlargest(10, 'avg_volume')\n",
    "    sns.barplot(x='ticker', y='avg_volume', data=top_10_volume, ax=axs[1, 1])\n",
    "    axs[1, 1].set_title('Top 10 Stocks by Average Trading Volume')\n",
    "\n",
    "    sns.heatmap(accumulated_data['country_growth'].pivot(columns='country', values='total_volume'), ax=axs[2, 0], cmap='YlOrRd')\n",
    "    axs[2, 0].set_title('Company Growth by Country')\n",
    "\n",
    "    sns.heatmap(accumulated_data['tech_growth'].pivot(columns='industry', values='total_volume'), ax=axs[2, 1], cmap='YlOrRd')\n",
    "    axs[2, 1].set_title('Technology Market Growth')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "267e4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_summary(df):\n",
    "    global total_transactions, unique_countries, unique_tickers, unique_industries, unique_sectors, total_volume\n",
    "    \n",
    "    # Update total transactions\n",
    "    batch_transactions = df.count()\n",
    "    total_transactions += batch_transactions\n",
    "    \n",
    "    # Update unique sets\n",
    "    unique_countries.update(df.select('country').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "    unique_tickers.update(df.select('ticker').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "    unique_industries.update(df.select('industry').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "    unique_sectors.update(df.select('sector').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "    \n",
    "    # Update total volume\n",
    "    batch_volume = df.agg(sum('volume')).collect()[0][0]\n",
    "    total_volume += batch_volume\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Total transactions processed: {total_transactions}\")\n",
    "    print(f\"Number of unique countries: {len(unique_countries)}\")\n",
    "    print(f\"Number of unique tickers: {len(unique_tickers)}\")\n",
    "    print(f\"Number of unique industries: {len(unique_industries)}\")\n",
    "    print(f\"Number of unique sectors: {len(unique_sectors)}\")\n",
    "    print(f\"Total market volume: {total_volume}\")\n",
    "    \n",
    "    # Additional detailed information\n",
    "    print(\"\\nTop 5 countries by transaction count:\")\n",
    "    country_counts = Counter(df.select('country').rdd.flatMap(lambda x: x).collect())\n",
    "    for country, count in country_counts.most_common(5):\n",
    "        print(f\"{country}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 5 tickers by volume:\")\n",
    "    ticker_volumes = df.groupBy('ticker').agg(sum('volume').alias('total_volume')).orderBy('total_volume', ascending=False).limit(5).collect()\n",
    "    for row in ticker_volumes:\n",
    "        print(f\"{row['ticker']}: {row['total_volume']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9183f4e0166f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each batch\n",
    "def process_batch(df, epoch_id):\n",
    "    try:\n",
    "        print(f\"Processing batch {epoch_id}\")\n",
    "        print(\"Input DataFrame Schema:\")\n",
    "        df.printSchema()\n",
    "\n",
    "        print(\"\\nInput DataFrame Sample:\")\n",
    "        df.show(5, truncate=False)\n",
    "\n",
    "        if df.rdd.isEmpty():\n",
    "            print(\"Warning: Input DataFrame is empty. Skipping this batch.\")\n",
    "            return\n",
    "\n",
    "        # Enrich the data with static information\n",
    "        enriched_df = df.join(tickers_df, \"ticker\", \"left_outer\")\n",
    "        print(\"\\nEnriched DataFrame Schema:\")\n",
    "        enriched_df.printSchema()\n",
    "\n",
    "        print(\"\\nEnriched DataFrame Sample:\")\n",
    "        enriched_df.show(5, truncate=False)\n",
    "        \n",
    "        print(f\"\\nUnique tickers in enriched data: {enriched_df.select('ticker').distinct().collect()}\")\n",
    "        print(f\"\\nUnique industries in enriched data: {enriched_df.select('industry').distinct().collect()}\")\n",
    "        print(f\"\\nUnique sectors in enriched data: {enriched_df.select('sector').distinct().collect()}\")\n",
    "        print(f\"\\nUnique countries in enriched data: {enriched_df.select('country').distinct().collect()}\")\n",
    "\n",
    "        # Count rows before and after join\n",
    "        input_count = df.count()\n",
    "        enriched_count = enriched_df.count()\n",
    "        print(f\"\\nRows before join: {input_count}\")\n",
    "        print(f\"Rows after join: {enriched_count}\")\n",
    "\n",
    "        if input_count != enriched_count:\n",
    "            print(\"WARNING: Row count changed after join. Some rows may have been dropped.\")\n",
    "\n",
    "        # Update summary\n",
    "        update_summary(enriched_df)\n",
    "\n",
    "        # Update plots\n",
    "        update_plots(enriched_df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {epoch_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0773dbe651b2702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the streaming query\n",
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Print the query status\n",
    "print(f\"Query started: {query.isActive}\")\n",
    "print(f\"Query name: {query.name}\")\n",
    "print(f\"Query id: {query.id}\")\n",
    "\n",
    "# Use awaitTermination\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping the streaming query...\")\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48e25229e2033b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
